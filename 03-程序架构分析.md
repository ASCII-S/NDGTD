# 03-程序架构分析模块

## 模块目标
理解NDGTD程序的整体架构，包括主程序流程、核心类设计、模块依赖关系和并行策略。

## 1. 程序整体架构

### 1.1 文件组织结构
```
NDGTD_3D_MPI/
├── src/                    # 源代码目录
│   ├── main.cpp           # 主程序入口
│   ├── NodeDgtd.h/cpp     # DGTD核心计算类
│   ├── MeshTET.h/cpp      # 四面体网格处理
│   ├── PreProcessor.h/cpp # 网格预处理器
│   ├── CElemSTD.h/cpp     # 参考单元数据
│   ├── comm.h/cpp         # MPI通信工具
│   ├── file.h/cpp         # 文件I/O处理
│   ├── cubature.h/cpp     # 数值积分
│   ├── datatype.h         # 数据结构定义
│   └── parameters.h       # 全局参数
├── runcase/               # 运行案例目录
└── NDGTD/                 # 文档目录
```

### 1.2 程序运行模式
NDGTD程序有两种运行模式：

#### 预处理模式 (定义__PREPROC__)
- **文件格式转换**：ANSYS文本格式到二进制格式
- **邻接关系分析**：生成INCM.BIN文件
- **数据准备**：为并行计算准备网格数据

#### 计算模式 (默认)
- **MPI初始化**：并行环境设置
- **网格分区**：域分解和数据分发
- **DGTD求解**：时域电磁场计算
- **结果输出**：诊断数据和场量数据

## 2. 主程序流程分析

### 2.1 main.cpp执行流程
```cpp
int main(int argc, char *argv[])
{
#ifdef __PREPROC__
    // 预处理模式
    if (__FILE_CONVERTOR__) {
        MeshFileFormatTxt2Bin();  // 文件格式转换
    }
    if (__PRE__PROCESSOR__) {
        CPreProcessor *ptr_Preprocessor = new CPreProcessor;  // 邻接关系分析
        delete ptr_Preprocessor;
    }
#else
    // 计算模式
    // 1. MPI初始化
    MPI_Init(&argc, &argv);
    MPI_Comm_size(MPI_COMM_WORLD, &num_proc_tot);
    MPI_Comm_rank(MPI_COMM_WORLD, &id_this_proc);
    
    // 2. 网格数据读取
    if (MASTER_PROCESS) {
        total_num_elems(num_elem_tot, __FILE__, __LINE__);
    }
    MPI_Bcast(&num_elem_tot, 1, MPI_INT, 0, MPI_COMM_WORLD);
    
    // 3. 创建网格实例
    CMeshTET *pMesh = new CMeshTET(id_this_proc, num_proc_tot, num_elem_tot);
    
    // 4. 创建DGTD计算实例
    CNodeDgtd theNodeDGTD(id_this_proc, num_proc_tot, num_elem_tot, pMesh);
    
    // 5. 主时间循环
    for (int it = 0; it < num_timestep_tot; it++) {
        theNodeDGTD.UpdateStepEH(it);  // DGTD时间步进
    }
    
    // 6. 清理资源
    MPI_Finalize();
#endif
    return 0;
}
```

### 2.2 关键执行阶段

#### 阶段1：MPI环境初始化
- **进程创建**：启动指定数量的MPI进程
- **进程编号分配**：每个进程获得唯一的进程ID
- **通信域建立**：建立MPI_COMM_WORLD通信域

#### 阶段2：网格数据分发
- **主进程读取**：进程0读取全局网格信息
- **数据广播**：将网格信息广播给所有进程
- **域分解**：根据进程数进行网格分区

#### 阶段3：计算实例创建
- **网格实例**：每个进程创建自己的网格分区
- **DGTD实例**：创建DGTD计算对象
- **数据拷贝**：从网格对象拷贝必要数据

#### 阶段4：时间循环求解
- **时间步进**：执行DGTD时间积分
- **通信同步**：进程间数据交换
- **结果输出**：定期输出诊断信息

## 3. 核心类设计分析

### 3.1 CNodeDgtd类 - DGTD计算核心

#### 类的主要职责
```cpp
class CNodeDgtd
{
public:
    // 构造函数：完成完整的初始化
    CNodeDgtd(int ip, int np, int ne, CMeshTET *pMesh);
    
    // 核心计算接口
    void UpdateStepEH(const int it);           // 时间步进
    void SwapInfo(const int it, const int irk); // 进程间通信
    void UpdatePML(int irk);                   // PML更新
    
    // 边界条件处理
    void HandleFaceTS(const double &tm);       // 总场散射场
    void ApplyCurrent(double t);               // 电流源
    
    // 后处理功能
    void RCS(double theta, double phi);        // RCS计算
    void ComplexFieldOnClosedSurface(double t1, double t2); // 复数场
    
private:
    // 基本进程信息
    int m_id_this_proc;    // 当前进程编号
    int m_num_proc_tot;    // 总进程数
    int m_num_my_elem_tot; // 本进程单元数
    
    // 核心数据容器
    std::vector<ELEM> m_vct_elems;  // 计算单元数据
    CElemSTD *m_pElemSTD;           // 参考单元数据
    
    // 并行通信数据
    std::vector<SWAP_IDS> m_vct_swap_ids;     // 通信索引
    std::vector<SWAP_UNIT> m_vct_swap_units;  // 通信数据
    
    // 特殊功能数据
    std::vector<PMLX> m_vct_pmlx;             // PML数据
    std::vector<TFSF_FACE> m_vct_tfsf_faces;  // 总散场面
};
```

#### 初始化流程
1. **基本信息设置**：进程号、网格指针等
2. **参考单元处理**：主进程读取并广播
3. **计算数据拷贝**：从网格对象获取必要数据
4. **并行通信准备**：建立MPI通信索引
5. **特殊区域识别**：PML、总散场、电流源等
6. **场量初始化**：设置初始条件

### 3.2 CMeshTET类 - 网格管理

#### 类的主要职责
```cpp
class CMeshTET
{
public:
    // 构造函数：网格分区和数据分发
    CMeshTET(int ip, int np, int ne);
    
    // 网格数据访问接口
    void GetElemData(int ie, ELEM& elem);     // 获取单元数据
    void GetNodeData(int in, R3PHY& node);    // 获取节点数据
    
private:
    // 网格数据
    std::vector<TET> m_vct_tets;      // 四面体单元
    std::vector<R3PHY> m_vct_nodes;   // 网格节点
    std::vector<INCM> m_vct_incm;     // 邻接关系
    
    // 分区信息
    int m_id_this_proc;               // 当前进程
    int m_num_proc_tot;               // 总进程数
    int m_num_elem_tot;               // 总单元数
};
```

#### 网格分区策略
- **均匀分区**：单元按进程数均匀分配
- **局部性优化**：相邻单元尽量分配给同一进程
- **负载均衡**：确保各进程计算负载平衡

### 3.3 CPreProcessor类 - 网格预处理

#### 类的主要职责
```cpp
class CPreProcessor
{
public:
    // 构造函数：执行邻接关系分析
    CPreProcessor();
    
private:
    // 邻接关系分析
    void AnalyzeAdjacency();          // 分析单元邻接关系
    void GenerateINCM();              // 生成INCM数据
    void WriteINCMFile();             // 写入INCM.BIN文件
};
```

#### 预处理功能
- **邻接关系分析**：识别单元间的拓扑关系
- **边界识别**：识别计算域边界
- **数据优化**：优化数据存储格式

## 4. 模块依赖关系

### 4.1 依赖关系图
```
main.cpp
├── NodeDgtd.h/cpp
│   ├── datatype.h
│   ├── MeshTET.h/cpp
│   ├── CElemSTD.h/cpp
│   └── comm.h/cpp
├── MeshTET.h/cpp
│   ├── datatype.h
│   └── file.h/cpp
├── PreProcessor.h/cpp
│   ├── datatype.h
│   └── file.h/cpp
└── file.h/cpp
    └── datatype.h
```

### 4.2 数据流向
```
网格文件 → PreProcessor → INCM.BIN
     ↓
MeshTET → 域分解 → 各进程网格分区
     ↓
NodeDgtd → DGTD计算 → 结果输出
```

### 4.3 通信模式
```
进程0 (主进程)
├── 读取全局网格数据
├── 广播基本信息
└── 分发网格分区

进程1-N (从进程)
├── 接收基本信息
├── 接收网格分区
└── 执行DGTD计算

所有进程
├── 边界数据交换
├── 同步时间步进
└── 结果收集
```

## 5. 并行策略分析

### 5.1 域分解策略

#### 数据分布
- **单元分布**：四面体单元按进程数均匀分配
- **节点分布**：节点数据根据需要复制到相关进程
- **边界处理**：边界单元需要特殊处理

#### 负载均衡
- **静态负载均衡**：基于单元数量的均匀分配
- **动态负载均衡**：运行时根据计算负载调整（未实现）
- **通信开销**：考虑通信成本进行分区优化

### 5.2 通信模式

#### 点对点通信
```cpp
// 边界数据交换
void CNodeDgtd::SwapInfo(const int it, const int irk)
{
    // 发送边界数据
    for (auto& swap_id : m_vct_swap_ids) {
        MPI_Isend(send_buffer, size, MPI_DOUBLE, 
                  swap_id.nip, tag, MPI_COMM_WORLD, &request);
    }
    
    // 接收边界数据
    for (auto& swap_id : m_vct_swap_ids) {
        MPI_Irecv(recv_buffer, size, MPI_DOUBLE, 
                  swap_id.nip, tag, MPI_COMM_WORLD, &request);
    }
    
    // 等待通信完成
    MPI_Waitall(requests.size(), requests.data(), statuses.data());
}
```

#### 集体通信
```cpp
// 全局同步
MPI_Barrier(MPI_COMM_WORLD);

// 数据广播
MPI_Bcast(&global_data, 1, MPI_INT, 0, MPI_COMM_WORLD);

// 数据收集
MPI_Gather(local_data, local_size, MPI_DOUBLE,
           global_data, local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);
```

### 5.3 通信优化

#### 通信重叠
- **非阻塞通信**：使用MPI_Isend/Irecv
- **计算通信重叠**：在通信进行时执行本地计算
- **批量传输**：减少通信次数

#### 通信模式优化
- **邻居通信**：只与相邻进程通信
- **数据打包**：减少通信数据量
- **通信调度**：优化通信顺序

## 6. 内存管理策略

### 6.1 内存分配
```cpp
// 动态分配计算单元
std::vector<ELEM> m_vct_elems;
m_vct_elems.resize(m_num_my_elem_tot);

// 预分配通信缓冲区
std::vector<SWAP_UNIT> m_vct_swap_units;
m_vct_swap_units.resize(m_vct_swap_ids.size());
```

### 6.2 内存优化
- **数据局部性**：相关数据组织在一起
- **缓存友好**：优化数据访问模式
- **内存池**：减少动态分配开销

### 6.3 资源管理
- **RAII原则**：构造函数分配，析构函数释放
- **智能指针**：自动内存管理
- **异常安全**：确保资源正确释放

## 7. 错误处理和调试

### 7.1 错误检查
```cpp
// MPI错误检查
if (MPI_Init(&argc, &argv) != MPI_SUCCESS) {
    std::cerr << "MPI initialization failed!" << std::endl;
    return EXIT_FAILURE;
}

// 内存分配检查
if (ptr_Preprocessor == NULL) {
    std::cerr << "Memory allocation failed!" << std::endl;
    std::exit(EXIT_FAILURE);
}
```

### 7.2 调试支持
- **断言**：使用assert进行条件检查
- **日志输出**：记录程序执行状态
- **性能分析**：使用PerformanceTimer类

### 7.3 异常处理
- **MPI异常**：处理通信错误
- **文件异常**：处理I/O错误
- **数值异常**：处理计算错误

## 8. 扩展性设计

### 8.1 模块化设计
- **功能分离**：每个类负责特定功能
- **接口设计**：清晰的类接口
- **依赖最小化**：减少模块间耦合

### 8.2 可配置性
- **参数化设计**：通过parameters.h配置
- **编译时选项**：通过宏定义控制功能
- **运行时配置**：通过命令行参数配置

### 8.3 可扩展性
- **新边界条件**：易于添加新的边界条件类型
- **新数值方法**：支持不同的时间积分方法
- **新输出格式**：支持不同的结果输出格式

## 9. 性能考虑

### 9.1 计算性能
- **向量化**：利用SIMD指令
- **循环优化**：优化内循环性能
- **算法优化**：选择高效的数值算法

### 9.2 内存性能
- **缓存优化**：提高缓存命中率
- **内存带宽**：优化内存访问模式
- **NUMA感知**：考虑NUMA架构

### 9.3 并行性能
- **负载均衡**：确保各进程负载平衡
- **通信优化**：减少通信开销
- **可扩展性**：支持大规模并行